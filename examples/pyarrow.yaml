input:
  python:
    name: batches
    serializer: none
    modules:
      - pyarrow
    script: |
      import pyarrow as pa
      from pyarrow import fs
      import pyarrow.compute as pc
      import pyarrow.dataset as ds
      
      gcs = fs.GcsFileSystem()
      
      filter = (pc.field("type") == "post") | (pc.field("type") == "comment")
      dataset = ds.dataset("rp-voutila-hackernews/hackernews/", format="parquet", filesystem=gcs).filter(filter)
      
      # need special handling to raw cython generators, so for now
      # wrap with a pure python generator
      def take_all():
        for batch in dataset.to_batches():
          yield batch
      batches = take_all()

pipeline:
  processors:
    - python:
        script: |
          # 'this' is a PyArrow RecordBatch
          root.nbytes = this.nbytes
          root.num_rows = this.num_rows

output:
  stdout: {}

http:
  enabled: false
